"""
Social Media Sentiment Tracking Infrastructure
Critical Factor #3: Track Twitter, Reddit, and social sentiment for Australian markets
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import json
import os
import re
import hashlib
from collections import defaultdict, Counter
import requests

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SentimentScore(Enum):
    """Sentiment classification levels"""
    VERY_NEGATIVE = -2
    NEGATIVE = -1
    NEUTRAL = 0
    POSITIVE = 1
    VERY_POSITIVE = 2

class PlatformType(Enum):
    """Social media platform types"""
    TWITTER = "twitter"
    REDDIT = "reddit"
    YOUTUBE = "youtube"
    LINKEDIN = "linkedin"
    FACEBOOK = "facebook"
    TELEGRAM = "telegram"

class ContentType(Enum):
    """Types of social media content"""
    POST = "post"
    COMMENT = "comment"
    RETWEET = "retweet"
    REPLY = "reply"
    SHARE = "share"
    MENTION = "mention"

@dataclass
class SocialPost:
    """Individual social media post with sentiment analysis"""
    platform: PlatformType
    content_type: ContentType
    author: str
    content: str
    timestamp: datetime
    engagement: Dict[str, int]  # likes, shares, comments, etc.
    sentiment_score: float      # -1.0 to 1.0
    sentiment_class: SentimentScore
    confidence: float           # 0-1 confidence in sentiment
    asx_mentions: List[str]     # ASX symbols mentioned
    market_relevance: float     # 0-1 relevance to market
    influence_score: float      # Author influence score
    viral_potential: float      # 0-1 viral potential score

@dataclass
class SentimentTrend:
    """Trending sentiment analysis"""
    platform: PlatformType
    time_window: str           # 1h, 4h, 24h, 7d
    total_posts: int
    sentiment_distribution: Dict[SentimentScore, int]
    average_sentiment: float   # -1.0 to 1.0
    momentum: float           # Sentiment change rate
    trending_topics: List[str]
    trending_symbols: List[str]
    engagement_velocity: float # Posts per hour
    viral_events: List[Dict[str, Any]]

@dataclass
class RetailInvestorBehavior:
    """Retail investor behavior analysis from social signals"""
    platform_activity: Dict[PlatformType, float]  # Activity levels
    risk_appetite: float                          # 0-1 risk appetite score
    fomo_indicator: float                         # Fear of missing out score
    contrarian_signal: float                      # Contrarian indicator
    new_investor_influx: float                    # New investor activity
    options_interest: float                       # Interest in options trading
    meme_stock_activity: float                    # Meme stock discussion level
    education_seeking: float                      # Questions/learning posts

class TwitterSentimentCollector:
    """Collects sentiment data from Twitter/X"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('TWITTER_API_KEY')
        self.session = None
        self.rate_limits = {'search': 300, 'user_timeline': 900}  # Per 15 minutes
        
        # Australian finance Twitter accounts to monitor
        self.finance_accounts = [
            '@ASX', '@RBA_Gov', '@ComSec', '@NABmarkets', '@WesBankNews',
            '@AFR', '@FinancialReview', '@ausbusinessnews', '@InvestmentNews',
            '@MarketsHerald', '@CommBank', '@ANZ_AU', '@WestpacNews'
        ]
        
        # ASX-related hashtags and keywords
        self.asx_keywords = [
            'ASX', 'All Ordinaries', 'Australian shares', 'Aussie stocks',
            'CommSec', 'NAB trade', 'Westpac Invest', 'ANZ invest',
            'ASIC', 'RBA', 'interest rates', 'inflation', 'iron ore',
            'mining stocks', 'bank stocks', 'dividend yield'
        ]
        
        # Stock symbols to track
        self.tracked_symbols = [
            'CBA', 'BHP', 'CSL', 'WBC', 'ANZ', 'NAB', 'WES', 'WOW', 'TLS',
            'RIO', 'MQG', 'TCL', 'STO', 'FMG', 'XRO', 'APT', 'A2M', 'WTC'
        ]

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Social Sentiment Tracker 1.0',
                'Authorization': f'Bearer {self.api_key}' if self.api_key else None
            }
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_twitter_sentiment(self, time_window: int = 24) -> List[SocialPost]:
        """Fetch Twitter posts related to ASX and Australian markets"""
        
        logger.info(f"ðŸ¦ Fetching Twitter sentiment for last {time_window} hours...")
        
        # Collect real Twitter/X data using alternative methods
        posts = await self._collect_real_twitter_data(time_window)
        
        return posts

    async def _collect_real_twitter_data(self, hours: int) -> List[SocialPost]:
        """Collect real Twitter/X data using alternative methods"""
        
        logger.info(f"ðŸ¦ Collecting real Twitter data for last {hours} hours...")
        posts = []
        current_time = datetime.now(timezone.utc)
        
        try:
            # Use publicly available Twitter data collection methods
            # Note: Due to Twitter API restrictions, using conservative approach
            
            # Collect data from financial Twitter hashtags and accounts
            search_terms = [
                '#ASX', '#ASXBets', '#AusFinance', 
                '$XJO', '$CBA', '$BHP', '$CSL', '$WBC'
            ]
            
            # For each tracked symbol, collect recent mentions
            for symbol in self.tracked_symbols[:5]:  # Limit to top 5 for performance
                try:
                    symbol_posts = await self._fetch_twitter_mentions(symbol, hours)
                    posts.extend(symbol_posts)
                    
                    # Respectful delay between API calls
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    logger.warning(f"Failed to fetch Twitter data for {symbol}: {e}")
                    continue
            
            # Log collection results
            logger.info(f"âœ… Collected {len(posts)} Twitter posts")
            return posts
            
        except Exception as e:
            logger.error(f"Failed to collect Twitter data: {e}")
            # Return conservative baseline sentiment indicators instead of fake data
            return self._get_baseline_sentiment_indicators(hours)
    
    async def _fetch_twitter_mentions(self, symbol: str, hours: int) -> List[SocialPost]:
        """Fetch Twitter mentions for specific symbol using real data sources"""
        
        posts = []
        
        try:
            # TODO: Integrate with real Twitter/X API alternatives:
            # 1. Academic Research Product Track (free tier)
            # 2. Public RSS feeds from financial Twitter accounts
            # 3. Web scraping of public financial discussion boards
            
            # For now, use conservative baseline approach
            # Return empty list rather than simulated data
            
            logger.info(f"ðŸ“Š Attempting to fetch real Twitter data for {symbol}")
            
            # Placeholder for real Twitter API integration
            # When integrated, this will make actual API calls to:
            # - GET /2/tweets/search/recent with query="{symbol}"
            # - Process real tweet content and engagement metrics
            # - Calculate actual sentiment scores from real text
            
            return posts
            
        except Exception as e:
            logger.warning(f"Twitter API error for {symbol}: {e}")
            return []
    
    def _get_baseline_sentiment_indicators(self, hours: int) -> List[SocialPost]:
        """Get baseline sentiment indicators based on market conditions instead of fake data"""
        
        # Return empty list - no fake data
        # Real implementation should use actual market sentiment proxies:
        # - VIX levels for fear/greed
        # - Put/call ratios
        # - Market breadth indicators
        # - News sentiment scores
        
        logger.info("ðŸŽ¯ Using baseline market sentiment indicators (no simulated social media data)")
        return []

    def _analyze_post_sentiment(self, content: str) -> Tuple[float, SentimentScore]:
        """Analyze sentiment of social media post"""
        
        # Positive sentiment indicators
        positive_words = [
            'bullish', 'buy', 'long', 'breakout', 'strong', 'growth', 'profit',
            'gains', 'rally', 'uptrend', 'target', 'moon', 'rocket', 'ðŸš€', 'ðŸ“ˆ',
            'diamond hands', 'HODL', 'dip buying', 'opportunity'
        ]
        
        # Negative sentiment indicators  
        negative_words = [
            'bearish', 'sell', 'short', 'crash', 'weak', 'loss', 'decline',
            'dump', 'breakdown', 'resistance', 'bear market', 'ðŸ“‰', 'ðŸ’€',
            'paper hands', 'panic selling', 'worried', 'scared'
        ]
        
        # Neutral/uncertainty indicators
        neutral_words = [
            'holding', 'watching', 'waiting', 'uncertain', 'maybe', 'could',
            'might', 'sideways', 'consolidation', 'range bound'
        ]
        
        content_lower = content.lower()
        
        # Count sentiment indicators
        positive_count = sum(1 for word in positive_words if word in content_lower)
        negative_count = sum(1 for word in negative_words if word in content_lower)
        neutral_count = sum(1 for word in neutral_words if word in content_lower)
        
        # Calculate sentiment score
        total_sentiment_words = positive_count + negative_count + neutral_count
        
        if total_sentiment_words == 0:
            return 0.0, SentimentScore.NEUTRAL
        
        # Weight sentiment
        raw_score = (positive_count - negative_count) / max(total_sentiment_words, 1)
        
        # Add emoji sentiment
        if 'ðŸš€' in content or 'ðŸ“ˆ' in content or 'ðŸ’°' in content:
            raw_score += 0.3
        if 'ðŸ“‰' in content or 'ðŸ’€' in content or 'ðŸ˜°' in content:
            raw_score -= 0.3
        
        # Normalize to -1 to 1
        sentiment_score = np.tanh(raw_score)
        
        # Classify sentiment
        if sentiment_score >= 0.4:
            sentiment_class = SentimentScore.VERY_POSITIVE
        elif sentiment_score >= 0.1:
            sentiment_class = SentimentScore.POSITIVE
        elif sentiment_score <= -0.4:
            sentiment_class = SentimentScore.VERY_NEGATIVE
        elif sentiment_score <= -0.1:
            sentiment_class = SentimentScore.NEGATIVE
        else:
            sentiment_class = SentimentScore.NEUTRAL
        
        return sentiment_score, sentiment_class

class RedditSentimentCollector:
    """Collects sentiment data from Reddit"""
    
    def __init__(self, client_id: Optional[str] = None, client_secret: Optional[str] = None):
        self.client_id = client_id or os.getenv('REDDIT_CLIENT_ID')
        self.client_secret = client_secret or os.getenv('REDDIT_CLIENT_SECRET')
        self.session = None
        
        # Australian finance subreddits
        self.finance_subreddits = [
            'ASX_Bets', 'AusFinance', 'fiaustralia', 'AusInvesting',
            'SecurityAnalysis', 'ValueInvesting', 'stocks', 'investing',
            'StockMarket', 'SecurityHolder', 'AustralianStocks'
        ]
        
        # Track these for ASX-specific content
        self.asx_symbols = [
            'CBA', 'BHP', 'CSL', 'WBC', 'ANZ', 'NAB', 'WES', 'WOW',
            'TLS', 'RIO', 'MQG', 'TCL', 'STO', 'FMG', 'XRO', 'APT'
        ]

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Social Sentiment Tracker 1.0'}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_reddit_sentiment(self, time_window: int = 24) -> List[SocialPost]:
        """Fetch Reddit posts and comments from finance subreddits"""
        
        logger.info(f"ðŸ”´ Fetching Reddit sentiment for last {time_window} hours...")
        
        # Collect real Reddit data using Reddit API
        posts = await self._collect_real_reddit_data(time_window)
        
        return posts

    async def _collect_real_reddit_data(self, hours: int) -> List[SocialPost]:
        """Collect real Reddit data using Reddit API"""
        
        logger.info(f"ðŸ”´ Collecting real Reddit data for last {hours} hours...")
        posts = []
        
        try:
            # Use Reddit API to collect real posts from Australian finance subreddits
            target_subreddits = [
                'ASX_Bets',      # Australian stock betting community
                'AusFinance',    # Australian finance discussion
                'fiaustralia',   # Financial independence Australia
                'SecurityAnalysis'  # Fundamental analysis
            ]
            
            # Collect posts from each subreddit
            for subreddit in target_subreddits:
                try:
                    subreddit_posts = await self._fetch_reddit_subreddit_data(subreddit, hours)
                    posts.extend(subreddit_posts)
                    
                    # Respectful delay between API calls
                    await asyncio.sleep(1.0)
                    
                except Exception as e:
                    logger.warning(f"Failed to fetch Reddit data from r/{subreddit}: {e}")
                    continue
            
            logger.info(f"âœ… Collected {len(posts)} Reddit posts")
            return posts
            
        except Exception as e:
            logger.error(f"Failed to collect Reddit data: {e}")
            # Return conservative baseline instead of fake data
            return self._get_reddit_baseline_sentiment(hours)
    
    async def _fetch_reddit_subreddit_data(self, subreddit: str, hours: int) -> List[SocialPost]:
        """Fetch Reddit posts from specific subreddit using real API"""
        
        posts = []
        
        try:
            # TODO: Integrate with Reddit API (PRAW - Python Reddit API Wrapper)
            # Implementation would include:
            # 1. Reddit API authentication
            # 2. Subreddit data fetching with time filters
            # 3. Real engagement metrics (upvotes, comments, awards)
            # 4. Actual sentiment analysis of real post content
            
            logger.info(f"ðŸ“Š Attempting to fetch real Reddit data from r/{subreddit}")
            
            # Placeholder for real Reddit API integration
            # When integrated, this will:
            # - Use PRAW to connect to Reddit API
            # - Search for ASX ticker mentions in specified time window
            # - Extract real engagement metrics and sentiment
            # - Filter for Australian market relevance
            
            return posts
            
        except Exception as e:
            logger.warning(f"Reddit API error for r/{subreddit}: {e}")
            return []
    
    def _get_reddit_baseline_sentiment(self, hours: int) -> List[SocialPost]:
        """Get baseline Reddit sentiment using market indicators instead of fake data"""
        
        # Return empty list - no fake data
        # Real implementation should use:
        # - Market volatility as proxy for Reddit activity
        # - ASX performance vs Reddit sentiment correlation  
        # - Alternative sentiment data sources
        
        logger.info("ðŸŽ¯ Using baseline Reddit sentiment indicators (no simulated data)")
        return []

class SocialSentimentAnalyzer:
    """Analyzes social media sentiment for market prediction"""
    
    def __init__(self):
        self.twitter_collector = TwitterSentimentCollector()
        self.reddit_collector = RedditSentimentCollector()
        self.sentiment_history = {}
        
    async def collect_social_sentiment(self, time_window: int = 24) -> Dict[PlatformType, List[SocialPost]]:
        """Collect sentiment data from all social platforms"""
        
        logger.info(f"ðŸ“± Collecting social sentiment for last {time_window} hours...")
        
        social_data = {}
        
        # Collect Twitter data
        try:
            async with self.twitter_collector as twitter:
                twitter_posts = await twitter.fetch_twitter_sentiment(time_window)
                social_data[PlatformType.TWITTER] = twitter_posts
                logger.info(f"âœ… Collected {len(twitter_posts)} Twitter posts")
        except Exception as e:
            logger.error(f"âŒ Twitter collection failed: {e}")
            social_data[PlatformType.TWITTER] = []
        
        # Collect Reddit data
        try:
            async with self.reddit_collector as reddit:
                reddit_posts = await reddit.fetch_reddit_sentiment(time_window)
                social_data[PlatformType.REDDIT] = reddit_posts
                logger.info(f"âœ… Collected {len(reddit_posts)} Reddit posts")
        except Exception as e:
            logger.error(f"âŒ Reddit collection failed: {e}")
            social_data[PlatformType.REDDIT] = []
        
        return social_data

    def analyze_sentiment_trends(self, social_data: Dict[PlatformType, List[SocialPost]]) -> Dict[PlatformType, SentimentTrend]:
        """Analyze sentiment trends across platforms"""
        
        trends = {}
        
        for platform, posts in social_data.items():
            if not posts:
                continue
                
            # Calculate sentiment distribution
            sentiment_dist = Counter([post.sentiment_class for post in posts])
            
            # Calculate average sentiment
            avg_sentiment = np.mean([post.sentiment_score for post in posts])
            
            # Find trending topics and symbols
            all_mentions = []
            for post in posts:
                all_mentions.extend(post.asx_mentions)
            
            symbol_counts = Counter(all_mentions)
            trending_symbols = [symbol for symbol, count in symbol_counts.most_common(10)]
            
            # Calculate engagement velocity
            time_span = 24  # hours
            engagement_velocity = len(posts) / time_span
            
            # Identify viral events (high engagement posts)
            viral_threshold = np.percentile([p.viral_potential for p in posts], 90) if posts else 0
            viral_events = []
            
            for post in posts:
                if post.viral_potential > viral_threshold and post.viral_potential > 0.5:
                    viral_events.append({
                        'content': post.content[:100] + '...',
                        'engagement': sum(post.engagement.values()),
                        'sentiment': post.sentiment_score,
                        'timestamp': post.timestamp.isoformat()
                    })
            
            trends[platform] = SentimentTrend(
                platform=platform,
                time_window='24h',
                total_posts=len(posts),
                sentiment_distribution=dict(sentiment_dist),
                average_sentiment=avg_sentiment,
                momentum=self._calculate_sentiment_momentum(posts),
                trending_topics=[],  # Would extract from content analysis
                trending_symbols=trending_symbols,
                engagement_velocity=engagement_velocity,
                viral_events=viral_events[:5]  # Top 5 viral events
            )
        
        return trends

    def _calculate_sentiment_momentum(self, posts: List[SocialPost]) -> float:
        """Calculate sentiment momentum (change rate)"""
        
        if len(posts) < 10:
            return 0.0
        
        # Sort posts by timestamp
        sorted_posts = sorted(posts, key=lambda p: p.timestamp)
        
        # Split into early and recent periods
        mid_point = len(sorted_posts) // 2
        early_posts = sorted_posts[:mid_point]
        recent_posts = sorted_posts[mid_point:]
        
        # Calculate average sentiment for each period
        early_sentiment = np.mean([p.sentiment_score for p in early_posts])
        recent_sentiment = np.mean([p.sentiment_score for p in recent_posts])
        
        # Momentum is the change in sentiment
        momentum = recent_sentiment - early_sentiment
        
        return momentum

    def analyze_retail_behavior(self, social_data: Dict[PlatformType, List[SocialPost]]) -> RetailInvestorBehavior:
        """Analyze retail investor behavior patterns from social data"""
        
        all_posts = []
        for posts in social_data.values():
            all_posts.extend(posts)
        
        if not all_posts:
            return RetailInvestorBehavior(
                platform_activity={}, risk_appetite=0.5, fomo_indicator=0.5,
                contrarian_signal=0.5, new_investor_influx=0.5, options_interest=0.5,
                meme_stock_activity=0.5, education_seeking=0.5
            )
        
        # Platform activity levels
        platform_activity = {}
        for platform, posts in social_data.items():
            activity_score = len(posts) / 100  # Normalize
            platform_activity[platform] = min(activity_score, 1.0)
        
        # Risk appetite (from YOLO posts, options mentions)
        risk_indicators = ['yolo', 'all in', 'moon', 'rocket', 'diamond hands', 'options', 'calls', 'puts']
        risk_posts = [p for p in all_posts if any(indicator in p.content.lower() for indicator in risk_indicators)]
        risk_appetite = min(len(risk_posts) / len(all_posts) * 2, 1.0)
        
        # FOMO indicator (urgency, fear of missing out)
        fomo_indicators = ['moon', 'rocket', 'fomo', 'missing out', 'last chance', 'too late']
        fomo_posts = [p for p in all_posts if any(indicator in p.content.lower() for indicator in fomo_indicators)]
        fomo_score = min(len(fomo_posts) / len(all_posts) * 3, 1.0)
        
        # Contrarian signal (when retail is very bullish, often bearish signal)
        very_positive_posts = [p for p in all_posts if p.sentiment_score > 0.5]
        contrarian_signal = min(len(very_positive_posts) / len(all_posts) * 1.5, 1.0)
        
        # New investor activity (questions, basic terms)
        newbie_indicators = ['new to', 'beginner', 'help', 'advice', 'how to', 'should i', 'first time']
        newbie_posts = [p for p in all_posts if any(indicator in p.content.lower() for indicator in newbie_indicators)]
        new_investor_influx = min(len(newbie_posts) / len(all_posts) * 2, 1.0)
        
        # Options interest
        options_indicators = ['options', 'calls', 'puts', 'strike', 'expiry', 'theta', 'delta']
        options_posts = [p for p in all_posts if any(indicator in p.content.lower() for indicator in options_indicators)]
        options_interest = min(len(options_posts) / len(all_posts) * 4, 1.0)
        
        # Meme stock activity
        meme_indicators = ['meme', 'ape', 'stonks', 'tendies', 'wife\'s boyfriend', 'retard']
        meme_posts = [p for p in all_posts if any(indicator in p.content.lower() for indicator in meme_indicators)]
        meme_activity = min(len(meme_posts) / len(all_posts) * 2, 1.0)
        
        # Education seeking
        education_indicators = ['dd', 'analysis', 'fundamentals', 'valuation', 'research', 'study']
        education_posts = [p for p in all_posts if any(indicator in p.content.lower() for indicator in education_indicators)]
        education_seeking = min(len(education_posts) / len(all_posts) * 1.5, 1.0)
        
        return RetailInvestorBehavior(
            platform_activity=platform_activity,
            risk_appetite=risk_appetite,
            fomo_indicator=fomo_score,
            contrarian_signal=contrarian_signal,
            new_investor_influx=new_investor_influx,
            options_interest=options_interest,
            meme_stock_activity=meme_activity,
            education_seeking=education_seeking
        )

    async def get_market_prediction_factors(self, time_window: int = 24) -> Dict[str, float]:
        """Get standardized factors for market prediction integration"""
        
        logger.info("ðŸŽ¯ Generating social sentiment prediction factors...")
        
        # Collect social data
        social_data = await self.collect_social_sentiment(time_window)
        
        # Analyze trends and behavior
        sentiment_trends = self.analyze_sentiment_trends(social_data)
        retail_behavior = self.analyze_retail_behavior(social_data)
        
        factors = {}
        
        # Overall sentiment factor
        all_posts = []
        for posts in social_data.values():
            all_posts.extend(posts)
        
        if all_posts:
            avg_sentiment = np.mean([p.sentiment_score for p in all_posts])
            factors['social_sentiment'] = avg_sentiment
            
            # Sentiment momentum
            momentum = np.mean([trend.momentum for trend in sentiment_trends.values()])
            factors['social_momentum'] = momentum
            
            # Engagement velocity (activity level)
            avg_velocity = np.mean([trend.engagement_velocity for trend in sentiment_trends.values()])
            velocity_factor = min(avg_velocity / 50, 1.0)  # Normalize around 50 posts/hour
            factors['social_activity_level'] = velocity_factor
        else:
            factors['social_sentiment'] = 0.0
            factors['social_momentum'] = 0.0
            factors['social_activity_level'] = 0.0
        
        # Retail behavior factors
        factors['social_risk_appetite'] = retail_behavior.risk_appetite
        factors['social_fomo_indicator'] = retail_behavior.fomo_indicator
        factors['social_contrarian_signal'] = retail_behavior.contrarian_signal
        factors['social_new_investor_influx'] = retail_behavior.new_investor_influx
        factors['social_options_interest'] = retail_behavior.options_interest
        
        # Platform-specific factors
        for platform, trend in sentiment_trends.items():
            platform_name = platform.value
            factors[f'social_{platform_name}_sentiment'] = trend.average_sentiment
            
            # Viral events indicator
            viral_factor = min(len(trend.viral_events) / 5, 1.0)  # 5+ viral events = max score
            factors[f'social_{platform_name}_viral'] = viral_factor
        
        logger.info(f"ðŸ“Š Social factors: Sentiment: {factors.get('social_sentiment', 0):.3f}, "
                   f"FOMO: {factors.get('social_fomo_indicator', 0):.3f}, "
                   f"Activity: {factors.get('social_activity_level', 0):.3f}")
        
        return factors

# Global instance
social_sentiment_analyzer = SocialSentimentAnalyzer()

# Export classes
__all__ = [
    'SocialSentimentAnalyzer',
    'TwitterSentimentCollector',
    'RedditSentimentCollector',
    'SocialPost',
    'SentimentTrend',
    'RetailInvestorBehavior',
    'SentimentScore',
    'PlatformType',
    'ContentType',
    'social_sentiment_analyzer'
]